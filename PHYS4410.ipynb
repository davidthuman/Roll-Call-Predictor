{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "S4ToJBloD0DM"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xml.dom.minidom as parser\n",
        "from bs4 import BeautifulSoup as bs\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Collection\n",
        "\n",
        "Roll-call votes are pulled from senate.gov and associated bills are pulled from congress.gov. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f61WyMCYjTrK",
        "outputId": "3d6fa3fe-2a0d-4e30-9c7a-619460437233"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "101 1: 10\n",
            "101 2: 20\n",
            "102 1: 30\n",
            "102 2: 40\n",
            "103 1: 50\n",
            "103 2: 60\n",
            "104 1: 70\n",
            "104 1: 80\n",
            "104 1: 90\n",
            "104 2: 100\n",
            "104 2: 110\n",
            "104 2: 120\n",
            "105 2: 130\n",
            "105 2: 140\n",
            "105 2: 150\n",
            "106 1: 160\n",
            "106 1: 170\n",
            "106 2: 180\n",
            "106 2: 190\n",
            "106 2: 200\n",
            "107 1: 210\n",
            "107 1: 220\n",
            "107 1: 230\n",
            "107 2: 240\n",
            "107 2: 250\n",
            "108 1: 260\n",
            "108 1: 270\n",
            "108 1: 280\n",
            "108 2: 290\n",
            "108 2: 300\n",
            "109 1: 310\n",
            "109 1: 320\n",
            "109 2: 330\n",
            "109 2: 340\n",
            "109 2: 350\n",
            "110 1: 360\n",
            "110 1: 370\n",
            "110 1: 380\n",
            "110 2: 390\n",
            "110 2: 400\n",
            "111 1: 410\n",
            "111 1: 420\n",
            "111 1: 430\n",
            "111 2: 440\n",
            "112 1: 450\n",
            "112 2: 460\n",
            "112 2: 470\n",
            "113 1: 480\n",
            "113 2: 490\n",
            "114 1: 500\n",
            "114 1: 510\n",
            "114 2: 520\n",
            "114 2: 530\n",
            "115 1: 540\n",
            "116 1: 550\n",
            "116 1: 560\n",
            "116 2: 570\n"
          ]
        }
      ],
      "source": [
        "total_bills = []\n",
        "total_votes = []\n",
        "count = 0\n",
        "\n",
        "for senate in range(101, 117):\n",
        "  for session in [1,2]:\n",
        "\n",
        "    response_0 = requests.get(f\"https://www.senate.gov/legislative/LIS/roll_call_lists/vote_menu_{senate}_{session}.xml\")\n",
        "    tree = parser.parseString(response_0.text)\n",
        "    votes = tree.documentElement.getElementsByTagName(\"vote\")\n",
        "\n",
        "    for vote in votes:\n",
        "      try:\n",
        "        issue = vote.getElementsByTagName(\"issue\")[0].childNodes[0].data.strip().split()\n",
        "        vote_number = vote.getElementsByTagName(\"vote_number\")[0].childNodes[0].data.strip()\n",
        "        question = vote.getElementsByTagName(\"question\")[0].childNodes[0].data.strip()\n",
        "        if (question != \"On Passage of the Bill\"):\n",
        "          continue\n",
        "        if (issue[0] in [\"S.\", \"H.R.\"]):\n",
        "\n",
        "          response_1 = requests.get(f\"https://www.senate.gov/legislative/LIS/roll_call_votes/vote{senate}{session}/vote_{senate}_{session}_{vote_number}.xml\")\n",
        "          tree = parser.parseString(response_1.text)\n",
        "          members = tree.documentElement.getElementsByTagName(\"member\")\n",
        "\n",
        "          roll_call = list(filter(lambda member: member.getElementsByTagName(\"last_name\")[0].childNodes[0].data.strip() == \"Leahy\", members))[0]\n",
        "          if (len(roll_call.getElementsByTagName(\"vote_cast\")) == 0):\n",
        "            continue\n",
        "          else:\n",
        "            roll = roll_call.getElementsByTagName(\"vote_cast\")[0].childNodes[0].data\n",
        "            if (roll == 'Yea'):\n",
        "              vote_b = 1\n",
        "            else:\n",
        "              vote_b = 0\n",
        "\n",
        "\n",
        "          if (issue[0] == \"S.\"):\n",
        "            congress = \"senate\"\n",
        "          else:\n",
        "            congress = \"house\"\n",
        "\n",
        "          response_2 = requests.get(f\"https://www.congress.gov/bill/{senate}th-congress/{congress}-bill/{issue[1]}/text?format=txt\")\n",
        "          try:\n",
        "            bill = bs(response_2.text).find(\"pre\").get_text()\n",
        "          except:\n",
        "            continue\n",
        "\n",
        "          count += 1\n",
        "          if (count % 10 == 0):\n",
        "            print(f\"{senate} {session}: {count}\")\n",
        "          total_bills.append(bill)\n",
        "          total_votes.append(vote_b)\n",
        "      except:\n",
        "        continue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reformatting the XML votes of 'Yea' or 'Nay' to a binary 1 or 0, respectively"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "hzTLejfxGqnY"
      },
      "outputs": [],
      "source": [
        "# reformat the roll-call votes\n",
        "\n",
        "votes = list(map(lambda vote: vote.getElementsByTagName(\"vote_cast\"), votes))\n",
        "\n",
        "Y_data = list(map(lambda vote: 1 if vote == \"Yea\" else 0, votes))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluating the Always-Yes predictive strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.819614711033275"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum(total_votes) / len(total_votes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Removing the zero-meaning words from all of the bills. Because we are not using sentence structure/word order, and just what words are present in the bill, this won't change the \"meaning\" of the bill, in the eyes of the neural network.\n",
        "\n",
        "Our zero-meaning words are pulled from two sources. The first is the top 100 most frequent unigram (single word) in the English language. General inspection was done to decide the cut-off from the list, as the top 100 is an arbitrary choice. The second is a list of English stopwords, which are defined to be words that carry no meaning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "# Getting negative data cleaning\n",
        "\n",
        "top_100_unigrams = pd.read_csv('frequency/unigram_freq.csv').iloc[:101]['word'].to_list()\n",
        "stopwords = json.loads(open('frequency/stop-words-english.json').read())\n",
        "\n",
        "null_words = set(top_100_unigrams + stopwords)\n",
        "\n",
        "for i, bill in enumerate(total_bills):\n",
        "    total_bills[i] = ' '.join(filter(lambda x: x.lower() not in null_words, bill.split()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Removing an special characters that come with text data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocessing\n",
        "def remove_string_special_characters(s):\n",
        "      \n",
        "    # removes special characters with ' '\n",
        "    stripped = re.sub('[^a-zA-z\\s]', '', s)\n",
        "    stripped = re.sub('_', '', stripped)\n",
        "      \n",
        "    # Change any white space to one space\n",
        "    stripped = re.sub('\\s+', ' ', stripped)\n",
        "      \n",
        "    # Remove start and end white spaces\n",
        "    stripped = stripped.strip()\n",
        "    if stripped != '':\n",
        "            return stripped.lower()\n",
        "\n",
        "bills = list(map(remove_string_special_characters, total_bills))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating a frequency dictionary for both unigrams and bigrams for all the words in all the bills."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the uni- and bi- gram frequencies\n",
        "\n",
        "bigram_freq = {}\n",
        "unigram_freq = {}\n",
        "\n",
        "for bill in bills:\n",
        "\n",
        "    for unigram in bill.split():\n",
        "        unigram_freq[unigram] = unigram_freq.get(unigram,0) + 1\n",
        "\n",
        "    for bigram in list(nltk.bigrams(bill.split())):\n",
        "        bigram_freq[bigram] = bigram_freq.get(bigram,0) + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sorting to get the top 4000 n-grams from all the bills."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the top K most frequent n-gram features\n",
        "K = 4000\n",
        "\n",
        "unigrams = list({key: val for key, val in sorted(unigram_freq.items(), key = lambda ele: ele[1], reverse = True)})[0:K]\n",
        "bigrams = list(map(lambda x: x[0] + ' ' + x[1], list({key: val for key, val in sorted(bigram_freq.items(), key = lambda ele: ele[1], reverse = True)})[0:K]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "def binary_feature_map(documents, features):\n",
        "    x = []\n",
        "    for doc in documents:\n",
        "        x1 = []\n",
        "        for feat in features:\n",
        "            if (doc.find(feat) != -1):\n",
        "                x1.append(1)\n",
        "            else:\n",
        "                x1.append(0)\n",
        "        x.append(x1)\n",
        "    return x\n",
        "\n",
        "def bag_feature_map(documents, features):\n",
        "    x = []\n",
        "    for doc in documents:\n",
        "        x1 = []\n",
        "        for feat in features:\n",
        "            x1.append(doc.count(feat))\n",
        "        x.append(x1 / np.linalg.norm(x1))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Processing each bill's text to create 4000-dimension vectors to represent the bill. For the binary features, we are seeing whether a unigram or bigram in found in the bill or not. For the bag-of-words, we are looking at the count of that unigram or bigram in that bill."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creating the binary features for the top unigrams for each bill\n",
        "X_uni_binary = binary_feature_map(bills, unigrams)\n",
        "\n",
        "# Creating the binary features for the top bigrams for each bill\n",
        "X_bi_binary = binary_feature_map(bills, bigrams)\n",
        "\n",
        "# Creating the bag-of-words count for the top unigrams for each bill\n",
        "X_uni_bag = bag_feature_map(bills, unigrams)\n",
        "\n",
        "# Creating the bag-of-words count for the top bigrams for each bill\n",
        "X_bi_bag = bag_feature_map(bills, bigrams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "def create_CSV(data, path):\n",
        "    with open(path, 'x') as f:\n",
        "        write = csv.writer(f)\n",
        "        write.writerows(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_votes = list(map(lambda x: [x], total_votes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "create_CSV(X_uni_binary, 'data/uni_binary_small.CSV')\n",
        "create_CSV(X_uni_bag, 'data/uni_bag_small.CSV')\n",
        "create_CSV(X_bi_binary, 'data/bi_binary_small.CSV')\n",
        "create_CSV(X_bi_bag, 'data/bi_bag_small.CSV')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "create_CSV(data_votes, 'data/votes_small.CSV')\n",
        "create_CSV(unigrams, 'data/unigrams_small.CSV')\n",
        "create_CSV(bigrams, 'data/bigrams_small.CSV')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Building the Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# importing the processed data\n",
        "\n",
        "X_uni_binary = pd.read_csv('data/uni_binary.CSV').to_numpy()\n",
        "X_bi_binary = pd.read_csv('data/bi_binary.CSV').to_numpy()\n",
        "X_uni_bag = pd.read_csv('data/uni_binary.CSV').to_numpy()\n",
        "X_bi_bag = pd.read_csv('data/bi_binary.CSV').to_numpy()\n",
        "\n",
        "Y_data = pd.read_csv('data/votes.CSV').to_numpy().flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_uni_binary_small = pd.read_csv('data/uni_binary_small.CSV').to_numpy()\n",
        "X_bi_binary_small = pd.read_csv('data/bi_binary_small.CSV').to_numpy()\n",
        "X_uni_bag_small = pd.read_csv('data/uni_binary_small.CSV').to_numpy()\n",
        "X_bi_bag_small = pd.read_csv('data/bi_binary_small.CSV').to_numpy()\n",
        "\n",
        "Y_data_small = pd.read_csv('data/votes_small.CSV').to_numpy().flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "def model_train_test_split(x_data, y_data):\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data)\n",
        "    return [x_train, x_test, y_train, y_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "k1NI3CMjFOMK"
      },
      "outputs": [],
      "source": [
        "# Unigram Binary Features\n",
        "unigram_binary = model_train_test_split(X_uni_binary_small, Y_data_small)\n",
        "\n",
        "# Bigram Binary Features\n",
        "bigram_binary = model_train_test_split(X_bi_binary_small, Y_data_small)\n",
        "\n",
        "# Unigram Bag-of-Words Features\n",
        "unigram_bag = model_train_test_split(X_uni_bag_small, Y_data_small)\n",
        "\n",
        "# Bigram Bag-of-Words Features\n",
        "bigram_bag = model_train_test_split(X_bi_bag_small, Y_data_small)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_dimensions(name, data):\n",
        "    print(name)\n",
        "    print('X_train:',np.shape(data[0]))\n",
        "    print('y_train:',np.shape(data[2]))\n",
        "    print('X_test:',np.shape(data[1]))\n",
        "    print('y_test:',np.shape(data[3]))\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unigram Binary\n",
            "X_train: (427, 4000)\n",
            "y_train: (427,)\n",
            "X_test: (143, 4000)\n",
            "y_test: (143,)\n",
            "\n",
            "\n",
            "Bigram Binary\n",
            "X_train: (427, 4000)\n",
            "y_train: (427,)\n",
            "X_test: (143, 4000)\n",
            "y_test: (143,)\n",
            "\n",
            "\n",
            "Unigram Bag\n",
            "X_train: (427, 4000)\n",
            "y_train: (427,)\n",
            "X_test: (143, 4000)\n",
            "y_test: (143,)\n",
            "\n",
            "\n",
            "Bigram Bag\n",
            "X_train: (427, 4000)\n",
            "y_train: (427,)\n",
            "X_test: (143, 4000)\n",
            "y_test: (143,)\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print_dimensions('Unigram Binary', unigram_binary)\n",
        "print_dimensions('Bigram Binary', bigram_binary)\n",
        "print_dimensions('Unigram Bag', unigram_bag)\n",
        "print_dimensions('Bigram Bag', bigram_bag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "xT9WP4PIFkVR"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.models import Sequential   # importing Sequential model\n",
        "from keras.layers import Dense        # importing Dense layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "Z-uxdGylHK5s"
      },
      "outputs": [],
      "source": [
        "def create_model():  \n",
        "    # declaring model\n",
        "\n",
        "    basic_model = Sequential()\n",
        "\n",
        "    # Adding layers to the model\n",
        "\n",
        "    # Input Layer\n",
        "    basic_model.add(Dense(units = 2000, activation = 'relu', input_dim = 4000))\n",
        "\n",
        "    # Hidden Layers\n",
        "    basic_model.add(Dense(units=2000, activation='relu', input_dim=4000))\n",
        "\n",
        "    \"\"\"\n",
        "    basic_model.add(Dense(units = 1000, activation = 'relu', input_dim = 2000))\n",
        "    basic_model.add(Dense(units = 500, activation = 'relu', input_dim = 1000))\n",
        "    basic_model.add(Dense(units=250, activation='relu', input_dim=500))\n",
        "    basic_model.add(Dense(units=125, activation='relu', input_dim=250))\n",
        "    basic_model.add(Dense(units = 63, activation = 'relu', input_dim = 125))\n",
        "    \"\"\"\n",
        "\n",
        "    # Output Layer\n",
        "    basic_model.add(Dense(1, activation = 'hard_sigmoid'))\n",
        "\n",
        "    # compiling the model\n",
        "    # opt = tf.keras.optimizers.Adam()\n",
        "    opt = tf.keras.optimizers.SGD(learning_rate=0.0001)\n",
        "\n",
        "    basic_model.compile(loss = 'binary_crossentropy', optimizer = opt, metrics = ['accuracy'])\n",
        "\n",
        "    return basic_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_test_model(data, model, epochs):\n",
        "\n",
        "    # training the model\n",
        "    model.fit(data[0], data[2], epochs=epochs)\n",
        "\n",
        "    # Test, Loss and accuracy\n",
        "    loss_and_metrics = model.evaluate(data[1], data[3])\n",
        "\n",
        "    print('Loss = ',loss_and_metrics[0])\n",
        "    print('Accuracy = ',loss_and_metrics[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "An Always-Yes model for the big data would give an accuracy of 0.6361980460341116\n",
        "An Always-Yes model for the small data would give an accuracy of 0.819614711033275"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "14/14 [==============================] - 1s 20ms/step - loss: 0.6436 - accuracy: 0.7260\n",
            "Epoch 2/10\n",
            "14/14 [==============================] - 0s 19ms/step - loss: 0.6040 - accuracy: 0.8009\n",
            "Epoch 3/10\n",
            "14/14 [==============================] - 0s 19ms/step - loss: 0.5744 - accuracy: 0.8103\n",
            "Epoch 4/10\n",
            "14/14 [==============================] - 0s 21ms/step - loss: 0.5510 - accuracy: 0.8126\n",
            "Epoch 5/10\n",
            "14/14 [==============================] - 0s 20ms/step - loss: 0.5330 - accuracy: 0.8103\n",
            "Epoch 6/10\n",
            "14/14 [==============================] - 0s 19ms/step - loss: 0.5200 - accuracy: 0.8126\n",
            "Epoch 7/10\n",
            "14/14 [==============================] - 0s 19ms/step - loss: 0.5102 - accuracy: 0.8150\n",
            "Epoch 8/10\n",
            "14/14 [==============================] - 0s 19ms/step - loss: 0.5037 - accuracy: 0.8150\n",
            "Epoch 9/10\n",
            "14/14 [==============================] - 0s 19ms/step - loss: 0.4987 - accuracy: 0.8150\n",
            "Epoch 10/10\n",
            "14/14 [==============================] - 0s 19ms/step - loss: 0.4949 - accuracy: 0.8150\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.4798 - accuracy: 0.8392\n",
            "Loss =  0.47976353764533997\n",
            "Accuracy =  0.8391608595848083\n"
          ]
        }
      ],
      "source": [
        "train_test_model(unigram_binary, create_model(), 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "142/142 [==============================] - 3s 16ms/step - loss: 0.6760 - accuracy: 0.6334\n",
            "Epoch 2/10\n",
            "142/142 [==============================] - 2s 16ms/step - loss: 0.6738 - accuracy: 0.6347\n",
            "Epoch 3/10\n",
            "142/142 [==============================] - 2s 16ms/step - loss: 0.6719 - accuracy: 0.6347\n",
            "Epoch 4/10\n",
            "142/142 [==============================] - 2s 16ms/step - loss: 0.6702 - accuracy: 0.6347\n",
            "Epoch 5/10\n",
            "142/142 [==============================] - 2s 17ms/step - loss: 0.6687 - accuracy: 0.6345\n",
            "Epoch 6/10\n",
            "142/142 [==============================] - 2s 16ms/step - loss: 0.6674 - accuracy: 0.6343\n",
            "Epoch 7/10\n",
            "142/142 [==============================] - 2s 17ms/step - loss: 0.6663 - accuracy: 0.6343\n",
            "Epoch 8/10\n",
            "142/142 [==============================] - 2s 17ms/step - loss: 0.6653 - accuracy: 0.6343\n",
            "Epoch 9/10\n",
            "142/142 [==============================] - 2s 17ms/step - loss: 0.6644 - accuracy: 0.6343\n",
            "Epoch 10/10\n",
            "142/142 [==============================] - 2s 17ms/step - loss: 0.6637 - accuracy: 0.6343\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.6630 - accuracy: 0.6417\n",
            "Loss =  0.6629984378814697\n",
            "Accuracy =  0.6417218446731567\n"
          ]
        }
      ],
      "source": [
        "train_test_model(bigram_binary, create_model(), 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "142/142 [==============================] - 3s 16ms/step - loss: 0.6650 - accuracy: 0.6327\n",
            "Epoch 2/10\n",
            "142/142 [==============================] - 2s 16ms/step - loss: 0.6634 - accuracy: 0.6327\n",
            "Epoch 3/10\n",
            "142/142 [==============================] - 2s 17ms/step - loss: 0.6623 - accuracy: 0.6327\n",
            "Epoch 4/10\n",
            "142/142 [==============================] - 2s 17ms/step - loss: 0.6616 - accuracy: 0.6327\n",
            "Epoch 5/10\n",
            "142/142 [==============================] - 2s 16ms/step - loss: 0.6609 - accuracy: 0.6327\n",
            "Epoch 6/10\n",
            "142/142 [==============================] - 2s 16ms/step - loss: 0.6604 - accuracy: 0.6327\n",
            "Epoch 7/10\n",
            "142/142 [==============================] - 2s 16ms/step - loss: 0.6599 - accuracy: 0.6327\n",
            "Epoch 8/10\n",
            "142/142 [==============================] - 2s 17ms/step - loss: 0.6594 - accuracy: 0.6327\n",
            "Epoch 9/10\n",
            "142/142 [==============================] - 2s 16ms/step - loss: 0.6589 - accuracy: 0.6327\n",
            "Epoch 10/10\n",
            "142/142 [==============================] - 2s 16ms/step - loss: 0.6585 - accuracy: 0.6327\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.6564 - accuracy: 0.6464\n",
            "Loss =  0.6563833355903625\n",
            "Accuracy =  0.6463575959205627\n"
          ]
        }
      ],
      "source": [
        "train_test_model(unigram_bag, create_model(), 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "142/142 [==============================] - 3s 16ms/step - loss: 0.7183 - accuracy: 0.3675\n",
            "Epoch 2/15\n",
            "142/142 [==============================] - 2s 16ms/step - loss: 0.7017 - accuracy: 0.4216\n",
            "Epoch 3/15\n",
            "142/142 [==============================] - 2s 17ms/step - loss: 0.6914 - accuracy: 0.5172\n",
            "Epoch 4/15\n",
            "142/142 [==============================] - 2s 16ms/step - loss: 0.6844 - accuracy: 0.5676\n",
            "Epoch 5/15\n",
            "142/142 [==============================] - 2s 17ms/step - loss: 0.6795 - accuracy: 0.5978\n",
            "Epoch 6/15\n",
            "142/142 [==============================] - 2s 16ms/step - loss: 0.6759 - accuracy: 0.6299\n",
            "Epoch 7/15\n",
            "142/142 [==============================] - 2s 17ms/step - loss: 0.6732 - accuracy: 0.6341\n",
            "Epoch 8/15\n",
            "142/142 [==============================] - 2s 16ms/step - loss: 0.6710 - accuracy: 0.6380\n",
            "Epoch 9/15\n",
            "142/142 [==============================] - 2s 17ms/step - loss: 0.6693 - accuracy: 0.6378\n",
            "Epoch 10/15\n",
            "142/142 [==============================] - 2s 17ms/step - loss: 0.6678 - accuracy: 0.6383\n",
            "Epoch 11/15\n",
            "142/142 [==============================] - 2s 17ms/step - loss: 0.6666 - accuracy: 0.6383\n",
            "Epoch 12/15\n",
            "142/142 [==============================] - 2s 17ms/step - loss: 0.6656 - accuracy: 0.6385\n",
            "Epoch 13/15\n",
            "142/142 [==============================] - 2s 17ms/step - loss: 0.6646 - accuracy: 0.6394\n",
            "Epoch 14/15\n",
            "142/142 [==============================] - 2s 17ms/step - loss: 0.6639 - accuracy: 0.6389\n",
            "Epoch 15/15\n",
            "142/142 [==============================] - 2s 17ms/step - loss: 0.6631 - accuracy: 0.6391\n",
            "48/48 [==============================] - 0s 6ms/step - loss: 0.6645 - accuracy: 0.6252\n",
            "Loss =  0.6645345091819763\n",
            "Accuracy =  0.625165581703186\n"
          ]
        }
      ],
      "source": [
        "train_test_model(bigram_bag, create_model(), 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 ('legislation-analysis': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "1ce31e4762255c15b1786a5cc6c5f334d71c89e3340089935f50c63bd39ebace"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
